{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f71c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, copy, os, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import itertools\n",
    "from scipy.stats import norm\n",
    "\n",
    "# translations for our datasets (full title + dimension D)\n",
    "dataset_descs = {\"avazu-app_binary_sparse\" : (\"Avazu (App)\", 1000000),\n",
    "                 \"avazu-site_binary_sparse\" : (\"Avazu (Site)\", 1000000),\n",
    "                 \"criteo_binary_sparse\" : (\"Criteo\", 1000000),\n",
    "                 \"dexter_binary_sparse\" : (\"Dexter\", 20000),\n",
    "                 \"dorothea_binary_sparse\" : (\"Dorothea\", 100000),\n",
    "                 \"kdd2010-a_binary_sparse\" : (\"KDD2010 (Algebra)\", 20216830),\n",
    "                 \"mnist8-4+9_binary_sparse\" : (\"MNIST8 (4+9)\", 784),\n",
    "                 \"news20_binary_sparse\" : (\"News20\", 1355191),\n",
    "                 \"newsgroups_binary_sparse\" : (\"Newsgroups (Binary, CS)\", 101631),\n",
    "                 \"pcmac_binary_sparse\" : (\"PCMAC\", 3289),\n",
    "                 \"rcv1_binary_sparse\" : (\"RCV1\", 47236),\n",
    "                 \"real-sim_binary_sparse\" : (\"Real-Sim\", 20958),\n",
    "                 \"sst2_binary_sparse\" : (\"SST-2\", 13757),\n",
    "                 \"url_binary_sparse\" : (\"URL\", 3231961),\n",
    "                 \"w8a_binary_sparse\" : (\"W8A\", 300),\n",
    "                 \"webspam_binary_sparse\" : (\"Webspam\", 254)}\n",
    "\n",
    "# which datasets are we working with?\n",
    "ordered_datasets = sorted(list(dataset_descs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6cef4",
   "metadata": {},
   "source": [
    "# Computational Costs Per Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da17b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Table of seconds per iteration of {PAC, FSOL} x {WRS, top-K, moving average, exponential average}:\n",
    "1. Standardize against WRS (dense + K=64) having 1 unit of time.\n",
    "2. Even if partial results, just note them.\n",
    "'''\n",
    "\n",
    "# create a dictionary to store our dataframes\n",
    "time_dfs = {}\n",
    "\n",
    "# go thru all of our base models\n",
    "for base_model in [\"PAC\", \"FSOL\"]:\n",
    "    \n",
    "    # create our table, where each row corresponds to a dataset\n",
    "    df = pd.DataFrame(data=None, columns=[\"Dataset\", \"D\", \"WRS\", \"Moving Avg.\", \"Expo. Avg.\"])\n",
    "    \n",
    "    # iterate thru our datasets\n",
    "    for dataset in ordered_datasets:\n",
    "        \n",
    "        # start our row with the name + D\n",
    "        row = list(dataset_descs[dataset])\n",
    "        \n",
    "        # WRS\n",
    "        WRS = np.nanmean(list(icfi([list(pd.read_csv(f\"WRS/results/{base_model}/{dataset}\"\n",
    "                                                     f\"/model={base_model}_ws=dense_K=64_seed={seed}_metrics.csv\")\\\n",
    "                                         .time_elapsed.values) for seed in range(5)])))\n",
    "        \n",
    "        # Moving-Avg.\n",
    "        MAG = np.nanmean(list(icfi([list(pd.read_csv(f\"moving_average/results/{base_model}/{dataset}\"\n",
    "                                                     f\"/model={base_model}_K=64_seed={seed}_metrics.csv\")\\\n",
    "                                         .time_elapsed.values) for seed in range(5)])))\n",
    "        \n",
    "        # Expo. Avg.\n",
    "        EAG = np.nanmean(list(icfi([list(pd.read_csv(f\"exponential_average/results/{base_model}/{dataset}\"\n",
    "                                                     f\"/model={base_model}_seed={seed}_metrics.csv\")\\\n",
    "                                         .time_elapsed.values) for seed in range(5)])))\n",
    "        \n",
    "        # add to our row + to our dataframe\n",
    "        row += [np.round(WRS/WRS, 3), np.round(MAG/WRS, 3), np.round(EAG/WRS, 3)]\n",
    "        df.loc[len(df.index)] = row\n",
    "        \n",
    "    # at the end, add to our dataframe dictionary\n",
    "    time_dfs[base_model] = df.sort_values(by=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d6e2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined dataframe to insert into the .tex file\n",
    "combined = pd.concat([time_dfs[\"PAC\"][[\"Dataset\", \"D\"]],\n",
    "                      time_dfs[\"PAC\"][[\"Moving Avg.\", \"Expo. Avg.\"]], \n",
    "                      time_dfs[\"FSOL\"][[\"Moving Avg.\", \"Expo. Avg.\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adb00c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "Dataset & D & Moving Avg. & Expo. Avg. & Moving Avg. & Expo. Avg. \\\\\n",
      "\\midrule\n",
      "Webspam & 254 & 0.953000 & 0.717000 & 0.738000 & 0.613000 \\\\\n",
      "W8A & 300 & 0.420000 & 0.353000 & 0.368000 & 0.336000 \\\\\n",
      "MNIST8 (4+9) & 784 & 1.125000 & 0.885000 & 0.968000 & 0.887000 \\\\\n",
      "PCMAC & 3289 & 0.483000 & 0.312000 & 0.377000 & 0.235000 \\\\\n",
      "SST-2 & 13757 & 0.903000 & 0.431000 & 0.748000 & 0.411000 \\\\\n",
      "Dexter & 20000 & 0.577000 & 0.143000 & 0.285000 & 0.066000 \\\\\n",
      "Real-Sim & 20958 & 0.949000 & 0.624000 & 0.707000 & 0.564000 \\\\\n",
      "RCV1 & 47236 & 1.825000 & 0.892000 & 1.366000 & 1.011000 \\\\\n",
      "Dorothea & 100000 & 1.194000 & 0.190000 & 0.580000 & 0.254000 \\\\\n",
      "Newsgroups (Binary, CS) & 101631 & 3.587000 & 1.042000 & 1.376000 & 0.543000 \\\\\n",
      "Avazu (App) & 1000000 & 8.357000 & 2.507000 & 8.078000 & 2.544000 \\\\\n",
      "Avazu (Site) & 1000000 & 18.704000 & 3.207000 & 11.612000 & 3.683000 \\\\\n",
      "Criteo & 1000000 & 14.267000 & 2.876000 & 7.009000 & 2.381000 \\\\\n",
      "News20 & 1355191 & 4.288000 & 1.356000 & 3.376000 & 1.153000 \\\\\n",
      "URL & 3231961 & 10.050000 & 3.945000 & 9.124000 & 4.058000 \\\\\n",
      "KDD2010 (Algebra) & 20216830 & 6.579000 & 2.402000 & 6.778000 & 2.301000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the full time complexity table for LaTeX\n",
    "print(combined.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4393e",
   "metadata": {},
   "source": [
    "# WRS vs. Moving Average vs. Exponential Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "816acee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test accuracies of {PAC} x {WRS, moving average, exponential average}:\n",
    "- 4x4 grid of subplots to cover 16 datasets.\n",
    "'''\n",
    "# take out the datasets that we don't have runs for\n",
    "datasets = [dataset for dataset in ordered_datasets]\n",
    "\n",
    "# for each base model ...\n",
    "for model in [\"PAC\"]:\n",
    "    \n",
    "    # custom zoom-in for each plot\n",
    "    ylims = {\"avazu-app_binary_sparse\" : 0.6,\n",
    "             \"avazu-site_binary_sparse\" : 0.75,\n",
    "             \"criteo_binary_sparse\" : 0.75,\n",
    "             \"dexter_binary_sparse\" : 0.6,\n",
    "             \"dorothea_binary_sparse\" : 0.85,\n",
    "             \"kdd2010-a_binary_sparse\" : 0.85,\n",
    "             \"mnist8-4+9_binary_sparse\" : 0.8,\n",
    "             \"news20_binary_sparse\": 0.6,\n",
    "             \"newsgroups_binary_sparse\" : 0.6,\n",
    "             \"pcmac_binary_sparse\" : 0.7,\n",
    "             \"rcv1_binary_sparse\" : 0.85,\n",
    "             \"real-sim_binary_sparse\" : 0.75,\n",
    "             \"sst2_binary_sparse\" : 0.6,\n",
    "             \"url_binary_sparse\" : 0.9,\n",
    "             \"w8a_binary_sparse\" : 0.9,\n",
    "             \"webspam_binary_sparse\" : 0.8}\n",
    "\n",
    "    # create a figure of 4x4 grid of subplots\n",
    "    fig, ax = plt.subplots(4, 4, dpi=200, figsize=(16, 12))\n",
    "\n",
    "    # go thru each dataset + plot the instantaneous test accuracy for base algorithm seed=0\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        \n",
    "        ##### INSTANTANEOUS METRICS\n",
    "        \n",
    "        # load in the instantaneous metrics for seed=0\n",
    "        if model == \"PAC\":\n",
    "\n",
    "            # immediately load in the best hyperparameters for this dataset + model\n",
    "            log10Cerr = pd.read_csv(\"WRS/base_variants/PAC_hparams.csv\")\\\n",
    "            .query(f\"dataset == '{dataset}'\")[[\"log10Cerr\"]].values[0,0]\n",
    "            log10Cerr = int(log10Cerr)\n",
    "\n",
    "            # load in the file\n",
    "            logs_inst = pd.read_csv(f\"hparam_tuning/results/{model}/{dataset}/model={model}_log10Cerr={log10Cerr}_seed=0_metrics.csv\")\n",
    "\n",
    "        elif model == \"FSOL\":\n",
    "\n",
    "            # immediately load in the best hyperparameters for this dataset + model\n",
    "            log2eta, log10lmbda = pd.read_csv(\"WRS/base_variants/FSOL_hparams.csv\")\\\n",
    "            .query(f\"dataset == '{dataset}'\")[[\"log2eta\", \"log10lmbda\"]].values[0]\n",
    "            log2eta, log10lmbda = log2eta, log10lmbda\n",
    "\n",
    "            # load in the file\n",
    "            logs_inst = pd.read_csv(f\"hparam_tuning/results/{model}/{dataset}/model={model}_log2eta={log2eta}_log10lmbda={log10lmbda}_seed=0_metrics.csv\")\n",
    "        \n",
    "        ##### ENSEMBLE METRICS\n",
    "        \n",
    "        # 1. WRS-augmented training (use K=64)\n",
    "        WRS = pd.read_csv(f\"WRS/results/{model}/{dataset}/model={model}_ws=dense_K=64_seed=0_metrics.csv\")\n",
    "        \n",
    "        # 2. SIMPLE-(MOVING) AVERAGE (use K=64)\n",
    "        SA = pd.read_csv(f\"moving_average/results/{model}/{dataset}/model={model}_K=64_seed=0_metrics.csv\")\n",
    "        \n",
    "        # 3. EXPONENTIAL-AVERAGE (just one setting, gamma=0.9)\n",
    "        EA = pd.read_csv(f\"exponential_average/results/{model}/{dataset}/model={model}_seed=0_metrics.csv\")\n",
    "        \n",
    "        #### PLOTTING\n",
    "        \n",
    "        ax[i // 4, i % 4].grid()\n",
    "        ax[i // 4, i % 4].plot(logs_inst[\"timestep\"], logs_inst[\"inst_test-set-acc\"], color=\"grey\", alpha=0.4, label=model)\n",
    "        ax[i // 4, i % 4].plot(WRS[\"timestep\"], WRS[\"WRS_test-set-acc_SA\"], color=\"black\", linewidth=1.0, alpha=1.0, label=\"WRS\")\n",
    "        ax[i // 4, i % 4].plot(WRS[\"timestep\"], WRS[\"WRS_test-set-acc_SA\"], color=\"yellow\", linewidth=5.0, alpha=0.5, label=\"WRS\")\n",
    "        ax[i // 4, i % 4].plot(SA[\"timestep\"], SA[\"SW_test-set-acc\"], color=\"blue\", linewidth=1.0, label=\"Moving Avg.\")\n",
    "        ax[i // 4, i % 4].plot(EA[\"timestep\"], EA[\"EW_test-set-acc\"], color=\"red\", linewidth=1.0, label=\"Expo. Avg.\")\n",
    "        \n",
    "        # show what the dataset size is too, reformat if necessary\n",
    "        ax[i // 4, i % 4].set_title(dataset_descs[dataset][0], fontsize=16)\n",
    "        ax[i // 4, i % 4].tick_params(\"both\", labelsize=13)\n",
    "        ax[i // 4, i % 4].set_ylim(bottom=ylims[dataset])\n",
    "    \n",
    "    # create our custom legend\n",
    "    custom_lines = [Line2D([0], [0], color=\"black\", linewidth=1.0, alpha=1.0, label=f\"WRS-Augmented Training\"),\n",
    "                    Line2D([0], [0], color=\"yellow\", linewidth=5.0, alpha=0.5, label=f\"WRS-Augmented Training (For Emphasis)\"),\n",
    "                    Line2D([0], [0], color=\"blue\", linewidth=1.0, label=\"Moving Avg.\"),\n",
    "                    Line2D([0], [0], color=\"red\", linewidth=1.0, label=\"Expo. Avg.\"),\n",
    "                    Line2D([0], [0], color=\"grey\", linestyle=None, \n",
    "                           label=model)]\n",
    "    fig.legend(handles=custom_lines, loc=\"lower center\", ncol=3, fontsize=16, bbox_to_anchor=(0.5, -0.075))\n",
    "    \n",
    "    # beautify\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/WRS+competitor-wrappers_PAC_final.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df749804",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test accuracies of {FSOL} x {WRS, top-K, moving average, exponential average}:\n",
    "- 4x4 grid of subplots to cover 16 datasets.\n",
    "'''\n",
    "# take out the datasets that we don't have runs for\n",
    "datasets = [dataset for dataset in ordered_datasets]\n",
    "\n",
    "# for each base model ...\n",
    "for model in [\"FSOL\"]:\n",
    "    \n",
    "    # custom zoom-in for each plot\n",
    "    ylims = {\"avazu-app_binary_sparse\" : 0.6,\n",
    "             \"avazu-site_binary_sparse\" : 0.75,\n",
    "             \"criteo_binary_sparse\" : 0.75,\n",
    "             \"dexter_binary_sparse\" : 0.6,\n",
    "             \"dorothea_binary_sparse\" : 0.85,\n",
    "             \"kdd2010-a_binary_sparse\" : 0.75,\n",
    "             \"mnist8-4+9_binary_sparse\" : 0.8,\n",
    "             \"news20_binary_sparse\": 0.6,\n",
    "             \"newsgroups_binary_sparse\" : 0.6,\n",
    "             \"pcmac_binary_sparse\" : 0.7,\n",
    "             \"rcv1_binary_sparse\" : 0.85,\n",
    "             \"real-sim_binary_sparse\" : 0.75,\n",
    "             \"sst2_binary_sparse\" : 0.6,\n",
    "             \"url_binary_sparse\" : 0.9,\n",
    "             \"w8a_binary_sparse\" : 0.6,\n",
    "             \"webspam_binary_sparse\" : 0.8}\n",
    "\n",
    "    # create a figure of 4x4 grid of subplots\n",
    "    fig, ax = plt.subplots(4, 4, dpi=200, figsize=(16, 12))\n",
    "\n",
    "    # go thru each dataset + plot the instantaneous test accuracy for base algorithm seed=0\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        \n",
    "        ##### INSTANTANEOUS METRICS\n",
    "        \n",
    "        # load in the instantaneous metrics for seed=0\n",
    "        if model == \"PAC\":\n",
    "\n",
    "            # immediately load in the best hyperparameters for this dataset + model\n",
    "            log10Cerr = pd.read_csv(\"WRS/base_variants/PAC_hparams.csv\")\\\n",
    "            .query(f\"dataset == '{dataset}'\")[[\"log10Cerr\"]].values[0,0]\n",
    "            log10Cerr = int(log10Cerr)\n",
    "\n",
    "            # load in the file\n",
    "            logs_inst = pd.read_csv(f\"hparam_tuning/results/{model}/{dataset}/model={model}_log10Cerr={log10Cerr}_seed=0_metrics.csv\")\n",
    "\n",
    "        elif model == \"FSOL\":\n",
    "\n",
    "            # immediately load in the best hyperparameters for this dataset + model\n",
    "            log2eta, log10lmbda = pd.read_csv(\"WRS/base_variants/FSOL_hparams.csv\")\\\n",
    "            .query(f\"dataset == '{dataset}'\")[[\"log2eta\", \"log10lmbda\"]].values[0]\n",
    "            log2eta, log10lmbda = log2eta, log10lmbda\n",
    "\n",
    "            # load in the file\n",
    "            logs_inst = pd.read_csv(f\"hparam_tuning/results/{model}/{dataset}/model={model}_log2eta={log2eta}_log10lmbda={log10lmbda}_seed=0_metrics.csv\")\n",
    "        \n",
    "        ##### ENSEMBLE METRICS\n",
    "        \n",
    "        # 1. WRS-augmented training (use K=64)\n",
    "        WRS = pd.read_csv(f\"WRS/results/{model}/{dataset}/model={model}_ws=dense_K=64_seed=0_metrics.csv\")\n",
    "        \n",
    "        # 2. SIMPLE-AVERAGE (use K=64)\n",
    "        SA = pd.read_csv(f\"moving_average/results/{model}/{dataset}/model={model}_K=64_seed=0_metrics.csv\")\n",
    "        \n",
    "        # 3. EXPONENTIAL-AVERAGE (just one setting, gamma=0.9)\n",
    "        EA = pd.read_csv(f\"exponential_average/results/{model}/{dataset}/model={model}_seed=0_metrics.csv\")\n",
    "        \n",
    "        #### PLOTTING\n",
    "        \n",
    "        ax[i // 4, i % 4].grid()\n",
    "        ax[i // 4, i % 4].plot(logs_inst[\"timestep\"], logs_inst[\"inst_test-set-acc\"], color=\"grey\", alpha=0.4, label=model)\n",
    "        ax[i // 4, i % 4].plot(WRS[\"timestep\"], WRS[\"WRS_test-set-acc_SA\"], color=\"black\", linewidth=1.0, alpha=1.0, label=\"WRS\")\n",
    "        ax[i // 4, i % 4].plot(WRS[\"timestep\"], WRS[\"WRS_test-set-acc_SA\"], color=\"yellow\", linewidth=5.0, alpha=0.5, label=\"WRS\")\n",
    "        ax[i // 4, i % 4].plot(SA[\"timestep\"], SA[\"SW_test-set-acc\"], color=\"blue\", linewidth=1.0, label=\"Moving Avg.\")\n",
    "        ax[i // 4, i % 4].plot(EA[\"timestep\"], EA[\"EW_test-set-acc\"], color=\"red\", linewidth=1.0, label=\"Expo. Avg.\")\n",
    "        \n",
    "        # show what the dataset size is too, reformat if necessary\n",
    "        ax[i // 4, i % 4].set_title(dataset_descs[dataset][0], fontsize=16)\n",
    "        ax[i // 4, i % 4].tick_params(\"both\", labelsize=13)\n",
    "        ax[i // 4, i % 4].set_ylim(bottom=ylims[dataset])\n",
    "    \n",
    "    # create our custom legend\n",
    "    custom_lines = [Line2D([0], [0], color=\"black\", linewidth=1.0, alpha=1.0, label=f\"WRS-Augmented Training\"),\n",
    "                    Line2D([0], [0], color=\"yellow\", linewidth=5.0, alpha=0.5, label=f\"WRS-Augmented Training (For Emphasis)\"),\n",
    "                    Line2D([0], [0], color=\"green\", linestyle=\"--\", linewidth=1.0, label=f\"Top-K\"),\n",
    "                    Line2D([0], [0], color=\"blue\", linewidth=1.0, label=\"Moving Avg.\"),\n",
    "                    Line2D([0], [0], color=\"red\", linewidth=1.0, label=\"Expo. Avg.\"),\n",
    "                    Line2D([0], [0], color=\"grey\", linestyle=None, \n",
    "                           label=model)]\n",
    "    fig.legend(handles=custom_lines, loc=\"lower center\", ncol=3, fontsize=16, bbox_to_anchor=(0.5, -0.075))\n",
    "    \n",
    "    # beautify\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/WRS+competitor-wrappers_FSOL_final.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeeb776",
   "metadata": {},
   "source": [
    "# WRS on ADAGRAD, SGD+Momentum, TGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e44628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test accuracies of {SGD+momentum} x {base, WRS}:\n",
    "- 4x4 grid of subplots to cover 16 datasets.\n",
    "'''\n",
    "# take out the datasets that we don't have runs for\n",
    "datasets = ordered_datasets\n",
    "\n",
    "# for each base model ...\n",
    "for model in [\"sgd+momentum\"]:\n",
    "    \n",
    "    # custom zoom-in for each plot\n",
    "    ylims = {\"avazu-app_binary_sparse\" : 0.8,\n",
    "             \"avazu-site_binary_sparse\" : 0.6,\n",
    "             \"criteo_binary_sparse\" : 0.6,\n",
    "             \"dexter_binary_sparse\" : 0.6,\n",
    "             \"dorothea_binary_sparse\" : 0.75,\n",
    "             \"kdd2010-a_binary_sparse\" : 0.75,\n",
    "             \"mnist8-4+9_binary_sparse\" : 0.82,\n",
    "             \"news20_binary_sparse\": 0.6,\n",
    "             \"newsgroups_binary_sparse\" : 0.6,\n",
    "             \"pcmac_binary_sparse\" : 0.6,\n",
    "             \"rcv1_binary_sparse\" : 0.93,\n",
    "             \"real-sim_binary_sparse\" : 0.85,\n",
    "             \"sst2_binary_sparse\" : 0.6,\n",
    "             \"url_binary_sparse\" : 0.9,\n",
    "             \"w8a_binary_sparse\" : 0.9,\n",
    "             \"webspam_binary_sparse\" : 0.8}\n",
    "\n",
    "    # create a figure of 4x4 grid of subplots\n",
    "    fig, ax = plt.subplots(4, 4, dpi=200, figsize=(16, 12))\n",
    "\n",
    "    # go thru each dataset + plot the instantaneous test accuracy for base algorithm seed=0\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        \n",
    "        ##### ENSEMBLE METRICS\n",
    "        \n",
    "        # load in our results for SEED=0\n",
    "        logs = pd.read_csv(f\"sgd_variants/results/{model}/{dataset}/model={model}_K=64_seed=0_metrics.csv\")\n",
    "        \n",
    "        #### PLOTTING\n",
    "        \n",
    "        ax[i // 4, i % 4].grid()\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"inst_test-set-acc\"], color=\"grey\", alpha=0.4, label=model)\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"WRS_test-set-acc\"], color=\"black\", linewidth=1.0, alpha=1.0, label=\"WRS\")\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"WRS_test-set-acc\"], color=\"yellow\", linewidth=5.0, alpha=0.5, label=\"WRS\")\n",
    "        \n",
    "        # show what the dataset size is too, reformat if necessary\n",
    "        ax[i // 4, i % 4].set_title(dataset_descs[dataset][0], fontsize=16)\n",
    "        ax[i // 4, i % 4].tick_params(\"both\", labelsize=13)\n",
    "        ax[i // 4, i % 4].set_ylim(bottom=ylims[dataset])\n",
    "    \n",
    "    # create our custom legend\n",
    "    custom_lines = [Line2D([0], [0], color=\"black\", linewidth=1.0, alpha=1.0, label=f\"WRS-Augmented Training\"),\n",
    "                    Line2D([0], [0], color=\"yellow\", linewidth=5.0, alpha=0.5, label=f\"WRS-Augmented Training (For Emphasis)\"),\n",
    "                    Line2D([0], [0], color=\"grey\", linestyle=None, label=\"SGD+Momentum\")]\n",
    "    fig.legend(handles=custom_lines, loc=\"lower center\", ncol=3, fontsize=16, bbox_to_anchor=(0.5, -0.075))\n",
    "    \n",
    "    # beautify\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/SGDM+WRS_final.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "173b66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test accuracies of {AdaGrad} x {base, WRS}:\n",
    "- 4x4 grid of subplots to cover 16 datasets.\n",
    "'''\n",
    "# take out the datasets that we don't have runs for\n",
    "datasets = ordered_datasets\n",
    "\n",
    "# for each base model ...\n",
    "for model in [\"adagrad\"]:\n",
    "    \n",
    "    # custom zoom-in for each plot\n",
    "    ylims = {\"avazu-app_binary_sparse\" : 0.88,\n",
    "             \"avazu-site_binary_sparse\" : 0.75,\n",
    "             \"criteo_binary_sparse\" : 0.75,\n",
    "             \"dexter_binary_sparse\" : 0.75,\n",
    "             \"dorothea_binary_sparse\" : 0.75,\n",
    "             \"kdd2010-a_binary_sparse\" : 0.85,\n",
    "             \"mnist8-4+9_binary_sparse\" : 0.93,\n",
    "             \"news20_binary_sparse\": 0.6,\n",
    "             \"newsgroups_binary_sparse\" : 0.6,\n",
    "             \"pcmac_binary_sparse\" : 0.6,\n",
    "             \"rcv1_binary_sparse\" : 0.93,\n",
    "             \"real-sim_binary_sparse\" : 0.85,\n",
    "             \"sst2_binary_sparse\" : 0.6,\n",
    "             \"url_binary_sparse\" : 0.96,\n",
    "             \"w8a_binary_sparse\" : 0.97,\n",
    "             \"webspam_binary_sparse\" : 0.8}\n",
    "\n",
    "    # create a figure of 4x4 grid of subplots\n",
    "    fig, ax = plt.subplots(4, 4, dpi=200, figsize=(16, 12))\n",
    "\n",
    "    # go thru each dataset + plot the instantaneous test accuracy for base algorithm seed=0\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        \n",
    "        ##### ENSEMBLE METRICS\n",
    "        \n",
    "        # load in our results for SEED=0\n",
    "        logs = pd.read_csv(f\"sgd_variants/results/{model}/{dataset}/model={model}_K=64_seed=0_metrics.csv\")\n",
    "        \n",
    "        #### PLOTTING\n",
    "        \n",
    "        ax[i // 4, i % 4].grid()\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"inst_test-set-acc\"], color=\"grey\", alpha=0.4, label=model)\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"WRS_test-set-acc\"], color=\"black\", linewidth=1.0, alpha=1.0, label=\"WRS\")\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"WRS_test-set-acc\"], color=\"yellow\", linewidth=5.0, alpha=0.5, label=\"WRS\")\n",
    "        \n",
    "        # show what the dataset size is too, reformat if necessary\n",
    "        ax[i // 4, i % 4].set_title(dataset_descs[dataset][0], fontsize=16)\n",
    "        ax[i // 4, i % 4].tick_params(\"both\", labelsize=13)\n",
    "        ax[i // 4, i % 4].set_ylim(bottom=ylims[dataset])\n",
    "    \n",
    "    # create our custom legend\n",
    "    custom_lines = [Line2D([0], [0], color=\"black\", linewidth=1.0, alpha=1.0, label=f\"WRS-Augmented Training\"),\n",
    "                    Line2D([0], [0], color=\"yellow\", linewidth=5.0, alpha=0.5, label=f\"WRS-Augmented Training (For Emphasis)\"),\n",
    "                    Line2D([0], [0], color=\"grey\", linestyle=None, label=\"AdaGrad\")]\n",
    "    fig.legend(handles=custom_lines, loc=\"lower center\", ncol=3, fontsize=16, bbox_to_anchor=(0.5, -0.075))\n",
    "    \n",
    "    # beautify\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/ADAGRAD+WRS_final.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba5423ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test accuracies of {Truncated Gradient} x {base, WRS}:\n",
    "- 4x4 grid of subplots to cover 16 datasets.\n",
    "'''\n",
    "# take out the datasets that we don't have runs for\n",
    "datasets = ordered_datasets\n",
    "\n",
    "# for each base model ...\n",
    "for model in [\"tgd\"]:\n",
    "    \n",
    "    # custom zoom-in for each plot\n",
    "    ylims = {\"avazu-app_binary_sparse\" : 0.85,\n",
    "             \"avazu-site_binary_sparse\" : 0.75,\n",
    "             \"criteo_binary_sparse\" : 0.75,\n",
    "             \"dexter_binary_sparse\" : 0.6,\n",
    "             \"dorothea_binary_sparse\" : 0.6,\n",
    "             \"kdd2010-a_binary_sparse\" : 0.8,\n",
    "             \"mnist8-4+9_binary_sparse\" : 0.6,\n",
    "             \"news20_binary_sparse\": 0.6,\n",
    "             \"newsgroups_binary_sparse\" : 0.6,\n",
    "             \"pcmac_binary_sparse\" : 0.6,\n",
    "             \"rcv1_binary_sparse\" : 0.85,\n",
    "             \"real-sim_binary_sparse\" : 0.77,\n",
    "             \"sst2_binary_sparse\" : 0.6,\n",
    "             \"url_binary_sparse\" : 0.6,\n",
    "             \"w8a_binary_sparse\" : 0.95,\n",
    "             \"webspam_binary_sparse\" : 0.5}\n",
    "\n",
    "    # create a figure of 4x4 grid of subplots\n",
    "    fig, ax = plt.subplots(4, 4, dpi=200, figsize=(16, 12))\n",
    "\n",
    "    # go thru each dataset + plot the instantaneous test accuracy for base algorithm seed=0\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        \n",
    "        ##### ENSEMBLE METRICS\n",
    "        \n",
    "        # load in our results for SEED=0\n",
    "        logs = pd.read_csv(f\"sgd_variants/results/{model}/{dataset}/model={model}_K=64_seed=0_metrics.csv\")\n",
    "        \n",
    "        #### PLOTTING\n",
    "        \n",
    "        ax[i // 4, i % 4].grid()\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"inst_test-set-acc\"], color=\"grey\", alpha=0.4, label=model)\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"WRS_test-set-acc\"], color=\"black\", linewidth=1.0, alpha=1.0, label=\"WRS\")\n",
    "        ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[\"WRS_test-set-acc\"], color=\"yellow\", linewidth=5.0, alpha=0.5, label=\"WRS\")\n",
    "        \n",
    "        # show what the dataset size is too, reformat if necessary\n",
    "        ax[i // 4, i % 4].set_title(dataset_descs[dataset][0], fontsize=16)\n",
    "        ax[i // 4, i % 4].tick_params(\"both\", labelsize=13)\n",
    "        ax[i // 4, i % 4].set_ylim(bottom=ylims[dataset])\n",
    "    \n",
    "    # create our custom legend\n",
    "    custom_lines = [Line2D([0], [0], color=\"black\", linewidth=1.0, alpha=1.0, label=f\"WRS-Augmented Training\"),\n",
    "                    Line2D([0], [0], color=\"yellow\", linewidth=5.0, alpha=0.5, label=f\"WRS-Augmented Training (For Emphasis)\"),\n",
    "                    Line2D([0], [0], color=\"grey\", linestyle=None, label=\"Truncated Gradient Descent (TGD)\")]\n",
    "    fig.legend(handles=custom_lines, loc=\"lower center\", ncol=3, fontsize=16, bbox_to_anchor=(0.5, -0.075))\n",
    "    \n",
    "    # beautify\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/TGD+WRS_final.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Afterburner)",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
