{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, copy, os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from utils_v2 import *\n",
    "\n",
    "# get our list of datasets\n",
    "datasets = sorted([d for d in load_data(\"QUERY\") if \"sparse\" in d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58d3dc",
   "metadata": {},
   "source": [
    "# Compute metrics on all FSOL variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ce64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what model are we working with?\n",
    "model = \"FSOL\"\n",
    "\n",
    "# create logs for all of our variants\n",
    "FSOL_logs = pd.DataFrame(data=None, columns=[\"model\", \"dataset\", \"log2eta\", \"log10lmbda\", \"seed\",\n",
    "                                             \"final_test-set-acc\", \"l1-regret\", \"l2-regret\"])\n",
    "\n",
    "# go thru each of our sparse datasets\n",
    "for dataset in tqdm(datasets):\n",
    "    \n",
    "    # get the filenames for this dataset\n",
    "    fnames = [f for f in sorted(os.listdir(f\"results/{model}/{dataset}\")) if \".csv\" in f]\n",
    "    \n",
    "    # iterate thru each filename\n",
    "    for fname in fnames:\n",
    "        \n",
    "        # unpack the settings + cast to appropriate types\n",
    "        model, log2eta, log10lmbda, seed = [s.split(\"=\")[1] for s in fname.split(\"_\")[:-1]]\n",
    "        log2eta, log10lmbda, seed = float(log2eta), float(log10lmbda), int(seed)\n",
    "        df = pd.read_csv(f\"results/{model}/{dataset}/{fname}\")\n",
    "        \n",
    "        # get our test accuracies + compute relevant metrics\n",
    "        test_accs, cum_max = df[\"inst_test-set-acc\"], df[\"inst_test-set-acc\"].cummax()\n",
    "        l1 = (cum_max - test_accs).mean()\n",
    "        l2 = np.sqrt((np.sign(cum_max - test_accs) * ((cum_max - test_accs) ** 2)).mean())\n",
    "        \n",
    "        # add to our logs\n",
    "        FSOL_logs.loc[len(FSOL_logs.index)] = [model, dataset, log2eta, log10lmbda, seed,\n",
    "                                               test_accs.values[-1], l1, l2]\n",
    "        \n",
    "# save our files as a .csv + then immediately average across 5x seeds so that we can compare better.\n",
    "FSOL_logs.to_csv(\"FSOL_performances.csv\", index=False)\n",
    "FSOL_logs = FSOL_logs.groupby([\"model\", \"dataset\", \"log2eta\", \"log10lmbda\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d826f",
   "metadata": {},
   "source": [
    "# Pick FSOL variants that we will use for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fe6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine our threshold (how much best performance can we deviate from to create adverse conditions?)\n",
    "threshold = 0.025\n",
    "\n",
    "# create dataframe of best hyperparameter variants for FSOL\n",
    "FSOL_hparams = pd.DataFrame(data=None, columns=[\"model\", \"dataset\", \"log2eta\", \"log10lmbda\", \n",
    "                                                \"test_acc\", \"l1_regret\", \"l2_regret\"])\n",
    "\n",
    "# go thru each of our datasets\n",
    "for dataset in datasets:\n",
    "\n",
    "    # get our highest possible final test accuracy\n",
    "    top_acc = FSOL_logs.query(f\"dataset == '{dataset}'\")[\"final_test-set-acc\"].max()\n",
    "    \n",
    "    # let's use the variant that got within 2.5% accuracy as the top acc, but with more oscillation to test.\n",
    "    model, dataset, log2eta, log10lmbda, _, test_acc, l1_regret, l2_regret = \\\n",
    "    FSOL_logs.query(f\"dataset == '{dataset}' and `final_test-set-acc` >= {top_acc - threshold}\")\\\n",
    "    .sort_values(by=\"l1-regret\", ascending=False).iloc[0]\n",
    "    \n",
    "    # add to our logs\n",
    "    FSOL_hparams.loc[len(FSOL_hparams.index)] = [model, dataset, log2eta, log10lmbda, \n",
    "                                                 test_acc, l1_regret, l2_regret]\n",
    "    \n",
    "# save our chosen hyperparameters\n",
    "FSOL_hparams.to_csv(\"FSOL_hparams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71dafab",
   "metadata": {},
   "source": [
    "# Compute metrics on all PAC variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69275d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what model are we working with?\n",
    "model = \"PAC\"\n",
    "\n",
    "# create logs for all of our variants\n",
    "PAC_logs = pd.DataFrame(data=None, columns=[\"model\", \"dataset\", \"log10Cerr\", \"seed\",\n",
    "                                            \"final_test-set-acc\", \"l1-regret\", \"l2-regret\"])\n",
    "\n",
    "# go thru each of our sparse datasets\n",
    "for dataset in tqdm(datasets):\n",
    "    \n",
    "    # get the filenames for this dataset\n",
    "    fnames = [f for f in sorted(os.listdir(f\"results/{model}/{dataset}\")) if \".csv\" in f]\n",
    "    \n",
    "    # iterate thru each filename\n",
    "    for fname in fnames:\n",
    "        \n",
    "        # unpack the settings + cast to appropriate types\n",
    "        model, log10Cerr, seed = [s.split(\"=\")[1] for s in fname.split(\"_\")[:-1]]\n",
    "        log10Cerr, seed = float(log10Cerr), int(seed)\n",
    "        df = pd.read_csv(f\"results/{model}/{dataset}/{fname}\")\n",
    "        \n",
    "        # get our test accuracies + compute relevant metrics\n",
    "        test_accs, cum_max = df[\"inst_test-set-acc\"], df[\"inst_test-set-acc\"].cummax()\n",
    "        l1 = (cum_max - test_accs).mean()\n",
    "        l2 = np.sqrt((np.sign(cum_max - test_accs) * ((cum_max - test_accs) ** 2)).mean())\n",
    "        \n",
    "        # add to our logs\n",
    "        PAC_logs.loc[len(PAC_logs.index)] = [model, dataset, log10Cerr, seed,\n",
    "                                             test_accs.values[-1], l1, l2]\n",
    "        \n",
    "# save our files as a .csv + then immediately average across 5x seeds so that we can compare better.\n",
    "PAC_logs.to_csv(\"PAC_performances.csv\", index=False)\n",
    "PAC_logs = PAC_logs.groupby([\"model\", \"dataset\", \"log10Cerr\"]).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7fb20",
   "metadata": {},
   "source": [
    "# Pick PAC variants that we will use for further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine our threshold (how much best performance can we deviate from to create adverse conditions?)\n",
    "threshold = 0.025\n",
    "\n",
    "# create dataframe of best hyperparameter variants for PAC\n",
    "PAC_hparams = pd.DataFrame(data=None, columns=[\"model\", \"dataset\", \"log10Cerr\", \n",
    "                                               \"test_acc\", \"l1_regret\", \"l2_regret\"])\n",
    "\n",
    "# go thru each of our datasets\n",
    "for dataset in datasets:\n",
    "\n",
    "    # get our highest possible final test accuracy\n",
    "    top_acc = PAC_logs.query(f\"dataset == '{dataset}'\")[\"final_test-set-acc\"].max()\n",
    "    \n",
    "    # let's use the variant that got within 2.5% accuracy as the top acc, but with more oscillation to test.\n",
    "    model, dataset, log10Cerr, _, test_acc, l1_regret, l2_regret = \\\n",
    "    PAC_logs.query(f\"dataset == '{dataset}' and `final_test-set-acc` >= {top_acc - threshold}\")\\\n",
    "    .sort_values(by=\"l1-regret\", ascending=False).iloc[0]\n",
    "    \n",
    "    # add to our logs\n",
    "    PAC_hparams.loc[len(PAC_hparams.index)] = [model, dataset, log10Cerr, \n",
    "                                               test_acc, l1_regret, l2_regret]\n",
    "    \n",
    "# save our chosen hyperparameters\n",
    "PAC_hparams.to_csv(\"PAC_hparams.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Afterburner)\n",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
