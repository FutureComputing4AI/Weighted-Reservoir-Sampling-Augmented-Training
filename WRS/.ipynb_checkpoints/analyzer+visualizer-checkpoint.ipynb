{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, copy, os, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# make a directory to store figures\n",
    "if \"figures\" not in os.listdir():\n",
    "    os.mkdir(\"figures\")\n",
    "    \n",
    "# translations for our datasets\n",
    "dataset_descs = {\"avazu-app_binary_sparse\" : \"Avazu (App)\",\n",
    "                 \"avazu-site_binary_sparse\" : \"Avazu (Site)\",\n",
    "                 \"criteo_binary_sparse\" : \"Criteo\",\n",
    "                 \"dexter_binary_sparse\" : \"Dexter\",\n",
    "                 \"dorothea_binary_sparse\" : \"Dorothea\",\n",
    "                 \"kdd2010-a_binary_sparse\" : \"KDD2010 (Algebra)\",\n",
    "                 \"mnist8-4+9_binary_sparse\" : \"MNIST8 (4+9)\",\n",
    "                 \"news20_binary_sparse\" : \"News20\",\n",
    "                 \"newsgroups_binary_sparse\" : \"Newsgroups (Binary, CS)\",\n",
    "                 \"pcmac_binary_sparse\" : \"PCMAC\",\n",
    "                 \"rcv1_binary_sparse\" : \"RCV1\",\n",
    "                 \"real-sim_binary_sparse\" : \"Real-Sim\",\n",
    "                 \"sst2_binary_sparse\" : \"SST-2\",\n",
    "                 \"url_binary_sparse\" : \"URL\",\n",
    "                 \"w8a_binary_sparse\" : \"W8A\",\n",
    "                 \"webspam_binary_sparse\" : \"Webspam\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4dfc36",
   "metadata": {},
   "source": [
    "# Generate a .csv summarizing all results + checking relative performance vs. base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71455acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for logs\n",
    "if \"logs\" not in os.listdir():\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "# create a dataframe to store all the logging results\n",
    "columns = [\"dataset\", \"model\", \"weight_scheme\", \"K\", \"seed\",\n",
    "           \"fin_test_acc_WA\", \"fin_test_hinge_WA\", \"fin_sparsity_WA\", \"L1_WA\",\n",
    "           \"fin_test_acc_WA_VZ\", \"fin_test_hinge_WA_VZ\", \"fin_sparsity_WA_VZ\", \"L1_WA_VZ\",\n",
    "           \"fin_test_acc_SA\", \"fin_test_hinge_SA\", \"fin_sparsity_SA\", \"L1_SA\",\n",
    "           \"fin_test_acc_SA_VZ\", \"fin_test_hinge_SA_VZ\", \"fin_sparsity_SA_VZ\", \"L1_SA_VZ\",\n",
    "           \"L1_inst\"]\n",
    "master = pd.DataFrame(data=None, columns=columns)\n",
    "\n",
    "\n",
    "# master table of results on our finished datasets\n",
    "for model in [\"PAC\", \"FSOL\"]:\n",
    "    for dataset in tqdm(list(dataset_descs.keys())):\n",
    "        \n",
    "        # get all filenames that are .csv and correspond to this model + dataset\n",
    "        fnames = sorted([f for f in os.listdir(f\"results/{model}/{dataset}\") if \".csv\" in f])\n",
    "        \n",
    "        # iterate thru all these filenames\n",
    "        for fname in fnames:\n",
    "            \n",
    "            # unpack our settings\n",
    "            model, weight_scheme, K, seed = [s.split(\"=\")[1] for s in fname.split(\"_\")[:-1]]\n",
    "            K, seed = int(K), int(seed)\n",
    "            \n",
    "            # start our row\n",
    "            row = [dataset, model, weight_scheme, K, seed]\n",
    "            \n",
    "            # get the corresponding instantaneous results for either PAC or FSOL\n",
    "            if model == \"PAC\":\n",
    "                \n",
    "                # immediately load in the best hyperparameters for this dataset + model\n",
    "                log10Cerr = pd.read_csv(\"base_variants/PAC_hparams.csv\")\\\n",
    "                .query(f\"dataset == '{dataset}'\")[[\"log10Cerr\"]].values[0,0]\n",
    "                log10Cerr = int(log10Cerr)\n",
    "                \n",
    "                # load in the file\n",
    "                logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log10Cerr={log10Cerr}_seed={seed}_metrics.csv\")\n",
    "                \n",
    "            elif model == \"FSOL\":\n",
    "                \n",
    "                # immediately load in the best hyperparameters for this dataset + model\n",
    "                log2eta, log10lmbda = pd.read_csv(\"base_variants/FSOL_hparams.csv\")\\\n",
    "                .query(f\"dataset == '{dataset}'\")[[\"log2eta\", \"log10lmbda\"]].values[0]\n",
    "                log2eta, log10lmbda = log2eta, log10lmbda\n",
    "                \n",
    "                # load in the file\n",
    "                logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log2eta={log2eta}_log10lmbda={log10lmbda}_seed={seed}_metrics.csv\")\n",
    "            \n",
    "            # load in the logs for this variant\n",
    "            logs = pd.read_csv(f\"results/{model}/{dataset}/{fname}\")\n",
    "            \n",
    "            # get the metrics that we are interested in\n",
    "            for a_type in [\"WA\", \"SA\"]:\n",
    "                for v_type in [\"\", \"_VZ\"]:\n",
    "                    \n",
    "                    # add the relevant columns to our row\n",
    "                    row += list(logs[[f\"WRS_test-set-acc_{a_type}{v_type}\", \n",
    "                                      f\"WRS_test-set-hinge_{a_type}{v_type}\", \n",
    "                                      f\"WRS_sparsity_{a_type}{v_type}\"]].iloc[-1].values)\n",
    "                    \n",
    "                    # compute the L1 metric + add to our list\n",
    "                    cm_inst_test_accs = logs_inst[\"inst_test-set-acc\"].cummax()\n",
    "                    test_accs = logs[f\"WRS_test-set-acc_{a_type}{v_type}\"]\n",
    "                    row += [(cm_inst_test_accs[1:] - test_accs[1:]).mean()]\n",
    "                    \n",
    "            # also need to store the L1 metric of the instantaneous solution\n",
    "            inst_test_accs = logs_inst[\"inst_test-set-acc\"]\n",
    "            row += [(cm_inst_test_accs[1:] - inst_test_accs[1:]).mean()]\n",
    "                    \n",
    "            # add to our dataframe\n",
    "            master.loc[len(master.index)] = row\n",
    "\n",
    "# at the very end\n",
    "master.to_csv(\"logs/master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38da82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each model, check how often we can beat the baselines in terms of L1, using K=64\n",
    "for model in [\"PAC\", \"FSOL\"]:\n",
    "    \n",
    "    # what model are we using?\n",
    "    print(f\"Model: {model} (K=64)\")\n",
    "    \n",
    "    ##########\n",
    "    \n",
    "    # check how many variants where we were able to beat the instantaneous baseline WITH EXPONENTIAL WEIGHTS\n",
    "    q = master.groupby([\"dataset\", \"model\", \"weight_scheme\", \"K\"]).mean().reset_index()\\\n",
    "    .sort_values(by=\"L1_SA\").query(f\"model == '{model}' and K == 64 and weight_scheme == 'exp-dense'\")\\\n",
    "    [[\"dataset\", \"model\", \"weight_scheme\", \"K\", \"L1_SA\", \"L1_WA\", \"L1_inst\"]]\n",
    "    \n",
    "    # how often did we beat the base model?\n",
    "    num_outperform_SA = ((q.L1_inst - q.L1_SA) > 0).sum()\n",
    "    print(f\"1. Simple-Average WRS with exponential weights stabilized baseline in {num_outperform_SA} of 16 datasets.\")\n",
    "    num_outperform_WA = ((q.L1_inst - q.L1_WA) > 0).sum()\n",
    "    print(f\"2. Weighted-Average WRS with exponential weights stabilized baseline in {num_outperform_WA} of 16 datasets.\")\n",
    "    \n",
    "    ##########\n",
    "    \n",
    "    # check how many variants where we were able to beat the instantaneous baseline WITH NON-EXPONENTIAL WEIGHTS\n",
    "    q = master.groupby([\"dataset\", \"model\", \"weight_scheme\", \"K\"]).mean().reset_index()\\\n",
    "    .sort_values(by=\"L1_SA\").query(f\"model == '{model}' and K == 64 and weight_scheme == 'dense'\")\\\n",
    "    [[\"dataset\", \"model\", \"weight_scheme\", \"K\", \"L1_SA\", \"L1_WA\", \"L1_inst\"]]\n",
    "    \n",
    "    # how often did we beat the base model?\n",
    "    num_outperform_SA = ((q.L1_inst - q.L1_SA) > 0).sum()\n",
    "    print(f\"3. Simple-Average WRS with standard weights stabilized baseline in {num_outperform_SA} of 16 datasets.\")\n",
    "    num_outperform_WA = ((q.L1_inst - q.L1_WA) > 0).sum()\n",
    "    print(f\"4. Weighted-Average WRS with standard weights stabilized baseline in {num_outperform_WA} of 16 datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986c2af",
   "metadata": {},
   "source": [
    "# Exhibition Figures - 2x2 of Avazu-App, News20 for PAC and FSOL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are our showcase datasets?\n",
    "showcase_datasets = [\"avazu-app_binary_sparse\", \"news20_binary_sparse\"]\n",
    "\n",
    "# let's go K=64, show all four possible options on ONE legend!\n",
    "fig, ax = plt.subplots(2, 2, dpi=200, figsize=(10, 7.5))\n",
    "\n",
    "# go thru our two models\n",
    "for i, model in enumerate([\"PAC\", \"FSOL\"]):\n",
    "    \n",
    "    # plot our two show case dataset\n",
    "    for j, dataset in enumerate(showcase_datasets):\n",
    "        \n",
    "        # load in the instantaneous test accuracies for seed=0\n",
    "        if model == \"PAC\":\n",
    "\n",
    "            # immediately load in the best hyperparameters for this dataset + model\n",
    "            log10Cerr = pd.read_csv(\"base_variants/PAC_hparams.csv\")\\\n",
    "            .query(f\"dataset == '{dataset}'\")[[\"log10Cerr\"]].values[0,0]\n",
    "            log10Cerr = int(log10Cerr)\n",
    "\n",
    "            # load in the file\n",
    "            logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log10Cerr={log10Cerr}_seed=0_metrics.csv\")\n",
    "\n",
    "        elif model == \"FSOL\":\n",
    "\n",
    "            # immediately load in the best hyperparameters for this dataset + model\n",
    "            log2eta, log10lmbda = pd.read_csv(\"base_variants/FSOL_hparams.csv\")\\\n",
    "            .query(f\"dataset == '{dataset}'\")[[\"log2eta\", \"log10lmbda\"]].values[0]\n",
    "            log2eta, log10lmbda = log2eta, log10lmbda\n",
    "\n",
    "            # load in the file\n",
    "            logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log2eta={log2eta}_log10lmbda={log10lmbda}_seed=0_metrics.csv\")\n",
    "        \n",
    "        # plot the instantaneous test accuracy + cumulative max. (Oracle)\n",
    "        ax[i, j].plot(logs_inst[\"timestep\"], logs_inst[\"inst_test-set-acc\"], alpha=0.4, color=\"grey\", label=model)\n",
    "        ax[i, j].plot(logs_inst[\"timestep\"], logs_inst[\"inst_test-set-acc\"].cummax(), color=\"black\", label=\"Oracle\")\n",
    "        \n",
    "        # load + plot the various WRS variants\n",
    "        logs = pd.read_csv(f\"results/{model}/{dataset}/model={model}_ws=dense_K=64_seed=0_metrics.csv\")\n",
    "        ax[i, j].plot(logs[\"timestep\"], logs[\"WRS_test-set-acc_SA\"], label=f\"{model}-WRS\", color=\"blue\")\n",
    "        \n",
    "        # deal with beautifying + labeling\n",
    "        ax[i, j].set_ylim(bottom=0.4)\n",
    "        ax[i, j].grid()\n",
    "        if j == 0:\n",
    "            ax[i, j].set_ylabel(model, fontsize=18)\n",
    "        if i == 0:\n",
    "            ax[i, j].set_title(dataset_descs[dataset], fontsize=18)\n",
    "        ax[i, j].tick_params(\"both\", labelsize=12)\n",
    "        ax[i, j].legend(loc=\"lower right\")\n",
    "        \n",
    "# beautify at the end\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/exhibition.png\", facecolor=\"white\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d5f8a",
   "metadata": {},
   "source": [
    "# Generating test accuracy and sparsity over time figures for each dataset, using $K=64$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e971f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory for K=64 metrics\n",
    "if \"K=64\" not in os.listdir(\"figures\"):\n",
    "    os.mkdir(\"figures/K=64\")\n",
    "\n",
    "# iterate thru these 3 sets of variables: only look at K=64.\n",
    "for model in [\"FSOL\", \"PAC\"]:\n",
    "    for weight_scheme in [\"dense\", \"exp-dense\"]:\n",
    "        for metric in [\"test-set-acc\", \"sparsity\"]:\n",
    "            \n",
    "            # start our figure\n",
    "            fig, ax = plt.subplots(4, 4, dpi=200, figsize=(14, 12))\n",
    "            \n",
    "            # just iterate thru all the datasets\n",
    "            for i, dataset in enumerate(dataset_descs.keys()):\n",
    "                \n",
    "                # load in the instantaneous test accuracies for seed=0\n",
    "                if model == \"PAC\":\n",
    "\n",
    "                    # immediately load in the best hyperparameters for this dataset + model\n",
    "                    log10Cerr = pd.read_csv(\"base_variants/PAC_hparams.csv\")\\\n",
    "                    .query(f\"dataset == '{dataset}'\")[[\"log10Cerr\"]].values[0,0]\n",
    "                    log10Cerr = int(log10Cerr)\n",
    "\n",
    "                    # load in the file\n",
    "                    logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log10Cerr={log10Cerr}_seed=0_metrics.csv\")\n",
    "\n",
    "                elif model == \"FSOL\":\n",
    "\n",
    "                    # immediately load in the best hyperparameters for this dataset + model\n",
    "                    log2eta, log10lmbda = pd.read_csv(\"base_variants/FSOL_hparams.csv\")\\\n",
    "                    .query(f\"dataset == '{dataset}'\")[[\"log2eta\", \"log10lmbda\"]].values[0]\n",
    "                    log2eta, log10lmbda = log2eta, log10lmbda\n",
    "\n",
    "                    # load in the file\n",
    "                    logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log2eta={log2eta}_log10lmbda={log10lmbda}_seed=0_metrics.csv\")\n",
    "                \n",
    "                # load + plot the various WRS variants\n",
    "                logs = pd.read_csv(f\"results/{model}/{dataset}/model={model}_ws={weight_scheme}_K=64_seed=0_metrics.csv\")\n",
    "                ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[f\"WRS_{metric}_SA\"], \n",
    "                                       label=f\"{model}-WRS (Simple Average)\", color=\"blue\", linewidth=0.75)\n",
    "                ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[f\"WRS_{metric}_SA_VZ\"], \n",
    "                                       label=f\"{model}-WRS (Simple Average + Voting-Based Zeroing)\", color=\"blue\", linestyle=\"--\", linewidth=0.75)\n",
    "                ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[f\"WRS_{metric}_WA\"], \n",
    "                                       label=f\"{model}-WRS (Weighted Average)\", color=\"red\", linewidth=0.75)\n",
    "                ax[i // 4, i % 4].plot(logs[\"timestep\"], logs[f\"WRS_{metric}_WA_VZ\"], \n",
    "                                       label=f\"{model}-WRS (Weighted Average + Voting-Based Zeroing)\", color=\"red\", linestyle=\"--\", linewidth=0.75)\n",
    "                \n",
    "                # plot the instantaneous test accuracy + cumulative max. (Oracle)\n",
    "                ax[i // 4, i % 4].plot(logs_inst[\"timestep\"], logs_inst[f\"inst_{metric}\"], alpha=0.4, color=\"grey\", label=model)\n",
    "                if metric != \"sparsity\":\n",
    "                    ax[i // 4, i % 4].plot(logs_inst[\"timestep\"], logs_inst[f\"inst_{metric}\"].cummax(), \n",
    "                                           color=\"black\", label=\"Oracle\", linewidth=1.0)\n",
    "            \n",
    "                # do some beautifying\n",
    "                ax[i // 4, i % 4].grid()\n",
    "                ax[i // 4, i % 4].tick_params(\"both\", labelsize=10)\n",
    "                if metric != \"sparsity\":\n",
    "                    ax[i // 4, i % 4].set_ylim(bottom=0.6)\n",
    "                ax[i // 4, i % 4].set_title(dataset_descs[dataset], fontsize=13.5)\n",
    "                ax[i // 4, i % 4].xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "            \n",
    "            # custom legend\n",
    "            custom_lines = [Line2D([0], [0], color=\"blue\", linestyle=None, \n",
    "                                   label=f\"{model}-WRS (Simple Average)\"),\n",
    "                            Line2D([0], [0], color=\"blue\", linestyle=\"--\", \n",
    "                                   label=f\"{model}-WRS (Simple Average + Voting-Based Zeroing)\"),\n",
    "                            Line2D([0], [0], color=\"red\", linestyle=None, \n",
    "                                   label=f\"{model}-WRS (Weighted Average)\"),\n",
    "                            Line2D([0], [0], color=\"red\", linestyle='--', \n",
    "                                   label=f\"{model}-WRS (Weighted Average + Voting-Based Zeroing)\"),\n",
    "                            Line2D([0], [0], color=\"grey\", linestyle=None, \n",
    "                                   label=model),\n",
    "                            Line2D([0], [0], color=\"black\", linestyle=None, \n",
    "                                   label=\"Oracle\")]\n",
    "            fig.legend(handles=custom_lines, loc=\"lower center\", ncol=3, fontsize=13.5, bbox_to_anchor=(0.5, -0.05))\n",
    "            \n",
    "            # beautify at the end\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"figures/K=64/{model}_ws={weight_scheme}_metric={metric}.png\", facecolor=\"white\",\n",
    "                        bbox_inches=\"tight\")\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Afterburner)\n",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
