{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff840118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, copy, os, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.stats import norm\n",
    "\n",
    "# make a directory to store figures\n",
    "if \"figures\" not in os.listdir():\n",
    "    os.mkdir(\"figures\")\n",
    "    \n",
    "# translations for our datasets\n",
    "dataset_descs = {\"avazu-app_binary_sparse\" : \"Avazu (App)\",\n",
    "                 \"avazu-site_binary_sparse\" : \"Avazu (Site)\",\n",
    "                 \"criteo_binary_sparse\" : \"Criteo\",\n",
    "                 \"dexter_binary_sparse\" : \"Dexter\",\n",
    "                 \"dorothea_binary_sparse\" : \"Dorothea\",\n",
    "                 \"kdd2010-a_binary_sparse\" : \"KDD2010 (Algebra)\",\n",
    "                 \"mnist8-4+9_binary_sparse\" : \"MNIST8 (4+9)\",\n",
    "                 \"news20_binary_sparse\" : \"News20\",\n",
    "                 \"newsgroups_binary_sparse\" : \"Newsgroups (Binary, CS)\",\n",
    "                 \"pcmac_binary_sparse\" : \"PCMAC\",\n",
    "                 \"rcv1_binary_sparse\" : \"RCV1\",\n",
    "                 \"real-sim_binary_sparse\" : \"Real-Sim\",\n",
    "                 \"sst2_binary_sparse\" : \"SST-2\",\n",
    "                 \"url_binary_sparse\" : \"URL\",\n",
    "                 \"w8a_binary_sparse\" : \"W8A\",\n",
    "                 \"webspam_binary_sparse\" : \"Webspam\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfb5db",
   "metadata": {},
   "source": [
    "# Generate summary of results as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affac237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory for logs\n",
    "if \"logs\" not in os.listdir():\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "# create a dataframe to store all the logging results\n",
    "columns = [\"dataset\", \"model\", \"K\", \"seed\",\n",
    "           \"fin_test_acc_WA\", \"fin_test_hinge_WA\", \"fin_sparsity_WA\", \"L1_WA\",\n",
    "           \"fin_test_acc_WA_VZ\", \"fin_test_hinge_WA_VZ\", \"fin_sparsity_WA_VZ\", \"L1_WA_VZ\",\n",
    "           \"fin_test_acc_SA\", \"fin_test_hinge_SA\", \"fin_sparsity_SA\", \"L1_SA\",\n",
    "           \"fin_test_acc_SA_VZ\", \"fin_test_hinge_SA_VZ\", \"fin_sparsity_SA_VZ\", \"L1_SA_VZ\",\n",
    "           \"L1_inst\"]\n",
    "master = pd.DataFrame(data=None, columns=columns)\n",
    "\n",
    "\n",
    "# master table of results on our finished datasets\n",
    "for model in [\"PAC\", \"FSOL\"]:\n",
    "    for dataset in tqdm(list(dataset_descs.keys())):\n",
    "        \n",
    "        # get all filenames that are .csv and correspond to this model + dataset\n",
    "        fnames = sorted([f for f in os.listdir(f\"results/TOPK/{model}/{dataset}\") if \".csv\" in f])\n",
    "        \n",
    "        # iterate thru all these filenames\n",
    "        for fname in fnames:\n",
    "            \n",
    "            # unpack our settings\n",
    "            model, K, seed = [s.split(\"=\")[1] for s in fname.replace(\"_TOPK\", \"\").split(\"_\")[:-1]]\n",
    "            K, seed = int(K), int(seed)\n",
    "            \n",
    "            # start our row\n",
    "            row = [dataset, model, K, seed]\n",
    "            \n",
    "            # get the corresponding instantaneous results for either PAC or FSOL\n",
    "            if model == \"PAC\":\n",
    "                \n",
    "                # immediately load in the best hyperparameters for this dataset + model\n",
    "                log10Cerr = pd.read_csv(\"base_variants/PAC_hparams.csv\")\\\n",
    "                .query(f\"dataset == '{dataset}'\")[[\"log10Cerr\"]].values[0,0]\n",
    "                log10Cerr = int(log10Cerr)\n",
    "                \n",
    "                # load in the file\n",
    "                logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log10Cerr={log10Cerr}_seed={seed}_metrics.csv\")\n",
    "                \n",
    "            elif model == \"FSOL\":\n",
    "                \n",
    "                # immediately load in the best hyperparameters for this dataset + model\n",
    "                log2eta, log10lmbda = pd.read_csv(\"base_variants/FSOL_hparams.csv\")\\\n",
    "                .query(f\"dataset == '{dataset}'\")[[\"log2eta\", \"log10lmbda\"]].values[0]\n",
    "                log2eta, log10lmbda = log2eta, log10lmbda\n",
    "                \n",
    "                # load in the file\n",
    "                logs_inst = pd.read_csv(f\"../hparam_tuning/results/{model}/{dataset}/model={model}_log2eta={log2eta}_log10lmbda={log10lmbda}_seed={seed}_metrics.csv\")\n",
    "            \n",
    "            # load in the logs for this variant\n",
    "            logs = pd.read_csv(f\"results/TOPK/{model}/{dataset}/{fname}\")\n",
    "            \n",
    "            # get the metrics that we are interested in\n",
    "            for a_type in [\"WA\", \"SA\"]:\n",
    "                for v_type in [\"\", \"_VZ\"]:\n",
    "                    \n",
    "                    # add the relevant columns to our row\n",
    "                    row += list(logs[[f\"TOPK_test-set-acc_{a_type}{v_type}\", \n",
    "                                      f\"TOPK_test-set-hinge_{a_type}{v_type}\", \n",
    "                                      f\"TOPK_sparsity_{a_type}{v_type}\"]].iloc[-1].values)\n",
    "                    \n",
    "                    # compute the L1 metric + add to our list\n",
    "                    cm_inst_test_accs = logs_inst[\"inst_test-set-acc\"].cummax()\n",
    "                    test_accs = logs[f\"TOPK_test-set-acc_{a_type}{v_type}\"]\n",
    "                    row += [(cm_inst_test_accs[1:] - test_accs[1:]).mean()]\n",
    "                    \n",
    "            # also need to store the L1 metric of the instantaneous solution\n",
    "            inst_test_accs = logs_inst[\"inst_test-set-acc\"]\n",
    "            row += [(cm_inst_test_accs[1:] - inst_test_accs[1:]).mean()]\n",
    "                    \n",
    "            # add to our dataframe\n",
    "            master.loc[len(master.index)] = row\n",
    "\n",
    "# at the very end\n",
    "master.to_csv(\"logs/topk_master.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beeb75b",
   "metadata": {},
   "source": [
    "# Number of Datasets Where Top-K beats Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5aaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each model, check how often we can beat the baselines in terms of L1, using K=64\n",
    "for model in [\"PAC\", \"FSOL\"]:\n",
    "    \n",
    "    # what model are we using?\n",
    "    print(f\"Model: {model} (K=64)\")\n",
    "    \n",
    "    ##########\n",
    "    \n",
    "    # check how many variants where we were able to beat the instantaneous baseline\n",
    "    q = master.groupby([\"dataset\", \"model\", \"K\"]).mean().reset_index()\\\n",
    "    .sort_values(by=\"L1_SA\").query(f\"model == '{model}' and K == 64\")\\\n",
    "    [[\"dataset\", \"model\", \"K\", \"L1_SA\", \"L1_WA\", \"L1_inst\"]]\n",
    "    \n",
    "    # how often did we beat the base model?\n",
    "    num_outperform_SA = ((q.L1_inst - q.L1_SA) > 0).sum()\n",
    "    print(f\"1. Simple-Average Top-K stabilized baseline in {num_outperform_SA} of 16 datasets.\")\n",
    "    num_outperform_WA = ((q.L1_inst - q.L1_WA) > 0).sum()\n",
    "    print(f\"2. Weighted-Average Top-K stabilized baseline in {num_outperform_WA} of 16 datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f859e",
   "metadata": {},
   "source": [
    "# Hypothesis Testing on Top-K vs. Base Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in our results for base PAC-II + FSOL\n",
    "base = pd.read_csv(\"../WRS/logs/baseline_logs.csv\")\n",
    "\n",
    "# how many datasets are we working with?\n",
    "N = 16\n",
    "\n",
    "# print for K=64\n",
    "print(\"Wilcoxon Signed-Rank Test for K=64 Top-K Variants on Relative Oracle Performance:\")\n",
    "\n",
    "# go thru all four variants at K=64\n",
    "for model in [\"PAC\", \"FSOL\"]:\n",
    "    for AS in [\"SA\", \"WA\"]:\n",
    "\n",
    "        # make our query\n",
    "        table = master.groupby([\"dataset\", \"model\", \"K\"]).mean().reset_index()\n",
    "\n",
    "        # get our model + top-K (treatment) and base model (control)\n",
    "        m_treat = table.query(f\"model == '{model}' and K == 64\")[f\"L1_{AS}\"].values\n",
    "        m_control = base.groupby([\"dataset\", \"model\"]).mean().reset_index().query(f\"model == '{model}'\")[\"L1_inst\"].values\n",
    "\n",
    "        # compute differences and then ranks -- this is TWO-SIDED TEST\n",
    "        d = m_treat - m_control\n",
    "        ranks = np.argsort(np.abs(d)) + 1\n",
    "\n",
    "        # compute test statistic\n",
    "        T = np.min([ranks[d > 0].sum() + 0.5*ranks[d == 0].sum(), \n",
    "                    ranks[d < 0].sum() + 0.5*ranks[d == 0].sum()])\n",
    "\n",
    "        # compute normal approximation\n",
    "        z = (T - 0.25*(N*(N+1))) / np.sqrt((1/24) * N * (N+1) * ( (2*N) + 1))\n",
    "\n",
    "        # get our p-value using normal approximation\n",
    "        pval = norm.cdf(z) * 2\n",
    "\n",
    "        print(f\"- {model}, {AS}: {pval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6149539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in our results for base PAC-II + FSOL\n",
    "base = pd.read_csv(\"../WRS/logs/baseline_logs.csv\")\n",
    "\n",
    "# how many datasets are we interested in?\n",
    "N = 16\n",
    "\n",
    "# print for K=64\n",
    "print(\"Wilcoxon Signed-Rank Test for K=64 Top-K Variants on Final Test Accuracy:\")\n",
    "\n",
    "# go thru all four variants at K=64\n",
    "for model in [\"PAC\", \"FSOL\"]:\n",
    "    for AS in [\"SA\", \"WA\"]:\n",
    "\n",
    "        # make our query\n",
    "        table = master.groupby([\"dataset\", \"model\", \"K\"]).mean().reset_index()\n",
    "\n",
    "        # get our model + top-K (treatment) and base model (control)\n",
    "        m_treat = table.query(f\"model == '{model}' and K == 64\")[f\"fin_test_acc_{AS}\"].values\n",
    "        m_control = base.groupby([\"dataset\", \"model\"]).mean().reset_index().query(f\"model == '{model}'\")[\"fin_test_acc_inst\"].values\n",
    "\n",
    "        # compute differences and then ranks -- this is TWO-SIDED TEST\n",
    "        d = m_treat - m_control\n",
    "        ranks = np.argsort(np.abs(d)) + 1\n",
    "\n",
    "        # compute test statistic\n",
    "        T = np.min([ranks[d > 0].sum() + 0.5*ranks[d == 0].sum(), \n",
    "                    ranks[d < 0].sum() + 0.5*ranks[d == 0].sum()])\n",
    "\n",
    "        # compute normal approximation\n",
    "        z = (T - 0.25*(N*(N+1))) / np.sqrt((1/24) * N * (N+1) * ( (2*N) + 1))\n",
    "\n",
    "        # get our p-value using normal approximation\n",
    "        pval = norm.cdf(z) * 2\n",
    "\n",
    "        print(f\"- {model}, {AS}: {pval}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Afterburner)\n",
   "language": "python",
   "name": "afterburner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
